{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a889b45",
   "metadata": {},
   "source": [
    "# Instacart Initial Data Exploration\n",
    "\n",
    "### Objective: \n",
    "        Import all data and clean the data sets so there are no duplicates and the data types are correct for the type of data stored.  There are 5 distinct data sets for this project and each will need to be processed.  Once the data is clean we will explore some relationships within the data and find interesting trends.\n",
    "        \n",
    "### Step 1:\n",
    "        First, we will import all the CSVs as Pandas dataframes, we expect the data to have issues with the import.  There are likely issues with the deliminators and possibly other issues as well.  We will first import and check the dataframe.  If there is an issue, we will re-import the file with new, more appropriate, settings until we get the data in the proper format. \n",
    "\n",
    "### Step 2:\n",
    "        Next, we will change the data type of each column in all dataframes as appropriate.  If there is an issue found here that requires us to re-import the file, that will be done here as well (an example would be the decimal is a \",\" not a \".\" and this was not found in step 1).  \n",
    "        \n",
    "### Step 3:\n",
    "        Next, we will find and remove duplicate values in the data so we do not skew the observations later in the process.  This will be done for each dataframe.  The goal is for each observation to have only one unique ID (the index).  This will allow us to join each dataframe later without having complications.  \n",
    "\n",
    "### Step 4:\n",
    "        Next, we will deal with Null or missing values in the data set.  This will only be important where the data is critical for the observations (For example missing values in a \"Notes\" column is not particularly important and to be expected).  The missing data will be dropped or interpolated when appropriate.  Considerations will be made at time of exploration for each instance of a column with missing data.  \n",
    "        \n",
    "### Step 5:\n",
    "        Finally, we will do some exploration of the now clean data and look for trends or patterns in the data to help with the business objectives.  In this case, we are trying to discover the shopping habits of customers.  Considerations will be made as we explore the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b77d5b",
   "metadata": {},
   "source": [
    "# Step 1: Importing Data\n",
    "\n",
    "***Initial Import to see if any special considerations are needed***\n",
    "We first need to see what the data looks like, the initial import will let the Pandas CSV reader interpret the data and then we will make any changes needed to prepare the data for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad36396d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6a1efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_insta_orders = pd.read_csv('/datasets/instacart_orders.csv')\n",
    "df_insta_orders.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7215008",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "\n",
    "The deliminator for the data is a semicolon so we will need to adjust the import statment, the decimal character looks to be good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8370c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_insta_orders = pd.read_csv('/datasets/instacart_orders.csv',sep=';')\n",
    "df_insta_orders.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d092c2db",
   "metadata": {},
   "source": [
    "#### Obersvations:\n",
    "\n",
    "The change to deliminator has worked and we have the columns properly imported.  The import stage of this data set is complete.\n",
    "\n",
    "***Next Data set:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2351e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_products = pd.read_csv('/datasets/products.csv')\n",
    "df_products.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9207bec",
   "metadata": {},
   "source": [
    "#### Observatons:\n",
    "\n",
    "It seems that all the data is using the semicolon (\";\") for the deliminator.  We will change the default for all future imports to be the semicolon.  This data set does not seem to have a decimal so we do not need to concider that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e2090f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_products = pd.read_csv('/datasets/products.csv',sep=';')\n",
    "df_products.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0606eee",
   "metadata": {},
   "source": [
    "####  Obersvations:\n",
    "The change to deliminator has worked and we have the columns properly imported. The import stage of this data set is complete.\n",
    "\n",
    "***Next Data set:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc81694",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aisles = pd.read_csv('/datasets/aisles.csv',sep=';')\n",
    "df_aisles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a3b693",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "\n",
    "The seperator has worked as desired, it seems that this dataset does not have any decimals as well so that is not a concideration.  The import stage of this data set is complete.\n",
    "\n",
    "***Next Data set:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e8793d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_departments = pd.read_csv('/datasets/departments.csv',sep=';')\n",
    "df_departments.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897eaae9",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "\n",
    "The import was as expected, the import stage of this data set is complete.\n",
    "\n",
    "***Final Data set:***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebac5186",
   "metadata": {},
   "source": [
    "#### Obersvations:\n",
    "The department_id is properly set as the index of the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdc679e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_order_products = pd.read_csv('/datasets/order_products.csv',sep=';')\n",
    "df_order_products.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7116f941",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "\n",
    "The import was as expected,  the import stage of this data set is complete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be814729",
   "metadata": {},
   "source": [
    "# Step 2: Checking and Setting Data Types\n",
    "\n",
    "***Note:***\n",
    "We will work with one dataframe at a time to keep the outputs from becoming to cluttered.  The order of dataframes will be the same as the oreder they were imported.  \n",
    "\n",
    "df_insta_orders\n",
    "\n",
    "df_products\n",
    "\n",
    "df_aisle\n",
    "\n",
    "df_departments\n",
    "\n",
    "df_order_product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9982ce",
   "metadata": {},
   "source": [
    "#### Process:\n",
    "\n",
    "We will start by calling .info() on the dataframe and check the datatypes.  If it looks like a datatype is misstyped as an object, we will attempt to change the data to the proper type.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7ded70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_insta_orders.info()\n",
    "df_insta_orders.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59975a59",
   "metadata": {},
   "source": [
    "***df_insta_orders***\n",
    "\n",
    "Looks good all data types are correct.  There are some missing values in the days since prior order column, this will be addressed later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802d8bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_products.info()\n",
    "df_products.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db29386",
   "metadata": {},
   "source": [
    "***df_products***\n",
    "\n",
    "Looks good all data types are correct.  There are some missing values in the product name cloumn.  This will be addressed later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b107439",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aisles.info()\n",
    "df_aisles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42caeee3",
   "metadata": {},
   "source": [
    "***df_aisle***\n",
    "\n",
    "Looks good all data types are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68c3408",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_departments.info()\n",
    "df_departments.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eeb400f",
   "metadata": {},
   "source": [
    "***df_department***\n",
    "\n",
    "Looks good all data types are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d455b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_order_products.info()\n",
    "df_order_products.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835b4b10",
   "metadata": {},
   "source": [
    "***df_order_product***\n",
    "\n",
    "Looks good all data types are correct.  It seems there are too many values to by default display the null values, this will be explored and addressed later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54bbd9e",
   "metadata": {},
   "source": [
    "# Step 3: Find and Remove Duplicate values\n",
    "\n",
    "##### Stragety:\n",
    "\n",
    "We are going to first run the duplicated() mehtod and the sum() method to count the number of duplicated rows.  If any entire rows are duplicated we are going to then drop the entire duplicated row. We will then check if there are duplicated values in key columns.  We will handle duplicated values in key columns depending on each unique situation.  This will be disucssed on a case by case basis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07357848-dc64-4156-9cc3-01ff4365226d",
   "metadata": {},
   "source": [
    "## Find and remove duplicate values (and describe why you make your choices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f179f0a9",
   "metadata": {},
   "source": [
    "### `insta_orders` data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99297a5-405a-463d-8535-9adc3da4ad74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicated orders and count the number\n",
    "df_insta_orders.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a3aafc",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "\n",
    "We seem to have 15 duplicated rows, next we will check the rows viusally before we drop them.  This will let us draw some conclusions as to what kind of rows are getting duplicated if there is a pattern.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af68b4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_insta_orders[df_insta_orders.duplicated(keep=False)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae1c90a",
   "metadata": {},
   "source": [
    "#### Observatons:\n",
    "\n",
    "It seems that only on Wensday at 2:00 am are we getting duplicated orders.  There may be an issue with the order taking/recording software.  This should be brough up with the subject matter expert to see if there is something odd happening here.  \n",
    "\n",
    "We will drop the duplicated values now.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3843612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the duplicates in place.\n",
    "df_insta_orders.drop_duplicates(inplace=True)\n",
    "\n",
    "# Count duplicates again to verify all of the duplicates are gone.  \n",
    "df_insta_orders.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e6bc46",
   "metadata": {},
   "source": [
    "### `products` data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f526b5b-8175-46fa-a0fd-441767d50e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for fully duplicate rows\n",
    "df_products.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8826b49",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "\n",
    "It seems like there are no duplicates.  Looks good, lets check if there are duplicate product names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371e88aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_products['product_name'].str.lower().duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd699451",
   "metadata": {},
   "source": [
    "#### Observatons:\n",
    "\n",
    "It seems some of the product names are duplicated.  Lets do a visual check to see what the duplicates look like.  If it looks like we can safely remove them we will just drop the duplicates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79f59d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prod_name_duplicate = df_products['product_name'].str.lower().duplicated(keep=False)\n",
    "df_products[df_prod_name_duplicate].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f69e6d",
   "metadata": {},
   "source": [
    "#### Observations:  \n",
    "\n",
    "It seems most of the duplicates were missing values, lets remove them and check what else is happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ce1c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_drop_na_duplicate = df_products[df_prod_name_duplicate].dropna()\n",
    "\n",
    "display(df_drop_na_duplicate.head(10))\n",
    "\n",
    "f\"number of total row pairs: {df_drop_na_duplicate['product_name'].count()}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42b9cc4",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "\n",
    "It seems that several product names were duplicated with only slight modifications like the \"Biotin 1000 mcg\" vs \"Biotin 1000 Mcg\" but each entry got a unique product_id.  The aisle number and department number are the same for this entry.  It is probably okay if we drop the duplicates, but we may end with product orders that do not have a matching product ID.  \n",
    "\n",
    "The best course of action would be to contact the database manager and fix the issue to have better data validation comming into the database.  For the purposes of our exploration, we will drop the duplicates as it should not affect the trends in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d27d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_products['product_name'] = df_products['product_name'].str.lower()\n",
    "\n",
    "df_products = df_products.drop_duplicates(subset=['product_name'])\n",
    "\n",
    "df_products.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40577b02",
   "metadata": {},
   "source": [
    "#### Observaions:\n",
    "\n",
    "Successfully removed the duplicated product names from the products dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f30db1",
   "metadata": {},
   "source": [
    "### `departments` data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb410ad4-0fbf-4b80-bb09-23fdea79afe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the duplicates for the dataframe\n",
    "df_departments.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c242a21",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "\n",
    "It seems there are not duplicated values in departments.  We can move on to the next dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889349c1",
   "metadata": {},
   "source": [
    "### `aisles` data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da89fb9-4679-40f5-ad0d-c34df753a3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aisles.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9823c65",
   "metadata": {},
   "source": [
    "#### Observations: \n",
    "\n",
    "It seems there are not duplicated values in aisles. we can move forward to the next dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274cd06a",
   "metadata": {},
   "source": [
    "### `order_products` data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52326689-84a8-4b8f-a881-7c68780f62c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check for fullly duplicate rows\n",
    "df_order_products.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7a84b2",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "\n",
    "There are no full rows duplicated, but we should check to see if there are duplicates in order_id and product_id pairs, since each order should only have one entry for each product.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1625519c",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = df_order_products.duplicated(subset=['order_id','product_id']).sum()\n",
    "\n",
    "display(f'Duplicated product_id, order_id pairs {count}')\n",
    "\n",
    "# We expect to see duplicated product ids and order ids so lets check for them to sanity check\n",
    "\n",
    "# Product ID\n",
    "count = df_order_products.duplicated(subset=['product_id']).sum()\n",
    "\n",
    "display(f'Duplicated product_ids {count}')\n",
    "\n",
    "# Order ID\n",
    "count = df_order_products.duplicated(subset=['order_id']).sum()\n",
    "\n",
    "display(f'Duplicated order_ids {count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1d0f92",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "\n",
    "The dataframe looks good.  There are no duplicated pairs of product/order id this suggests that all products are properly grouped per order.  We see there are a lot of duplicated order ids and product ids when we look at them individually, but this is expected.  Since each order has multiple products, we expect to see dupliacted order ids equal to the number of products in that order.  Similarly for product ids, the same product can be added in multiple orders, so it would make sense that there are a lot of duplicates. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcae2e9e",
   "metadata": {},
   "source": [
    "# Step 4: Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e46de76",
   "metadata": {},
   "source": [
    "## Find and remove missing values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb45c86",
   "metadata": {},
   "source": [
    "### `insta_orders` data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a784da63",
   "metadata": {},
   "source": [
    "#### Goal:\n",
    "\n",
    "We will explore the data and make sure that there is not missing or null values in the insta_orders dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166e5a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are there any missing values in the insta_orders dataframe?\n",
    "null_values = df_insta_orders.isnull().sum()\n",
    "\n",
    "print(f'number of null values:\\n{null_values}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbedf627",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "\n",
    "It seems there are a lrage number of missing values in the days_since_prior_order column.  Lets take a look at a random sample of 10 and see if we can see a pattern that will inform how to handle the null values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5a3272",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_insta_orders[df_insta_orders['days_since_prior_order'].isnull()].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efcd43c",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "\n",
    "It seems the majorty of null valuse come from first time orders (order_number = 1).  This would make sense there is no data for a prior order if this is the first order.  We should replace them with a number that would make sense.  Our choices are 0 or -1.  Any aggrigaton would be skewed by first orders so we would need to exclued them from our dataset anyway when doing that kind of annalysis.  I perfer -1 because it is obvious on first look that it is not possible and the value should be null.  For example a person could place 2 orders in one day and that would look exactly like a first time oreder with a null days_since_prior_order value if we used 0 to fill the nulls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af90859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the null values with -1\n",
    "df_insta_orders['days_since_prior_order'].fillna(-1,inplace=True)\n",
    "\n",
    "# Check to make sure there are no more null values\n",
    "display(df_insta_orders.isnull().sum())\n",
    "\n",
    "display(f\"The min value in the days_since_prior_order column: {df_insta_orders['days_since_prior_order'].min()}\")\n",
    "\n",
    "display(f\"The mix value in the days_since_prior_order column: {df_insta_orders['days_since_prior_order'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3628fe69",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "\n",
    "This looks good now, lets move on to the next dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dc9ca4",
   "metadata": {},
   "source": [
    "### `order_products` data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772f7199",
   "metadata": {},
   "source": [
    "#### Goal:\n",
    "\n",
    "We will explore the data and make sure that there is not missing or null values in the order_products dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8240253",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_order_products.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db0b2e4",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "\n",
    "We have some null values in the add_to_cart_order.  We first need to see some random data to try and identify some pattern in the null valuse we can use to handle them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e3df94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# boolean indexing of the null values to get the rows to sampel\n",
    "df_order_products[df_order_products['add_to_cart_order'].isnull()].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee46753",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "\n",
    "There dosen't seem to be a clear pattern in the data based on just random selected values.  We need to dig a bit deeper to find out why the values are null.  First we need to see some information on what the values in this column are normally.  It is possible that there was an odd value in the CSV for \"zero\" add to cart.  We will check some descriptive values for the coulmn.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53826611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets check the output of .describe()\n",
    "df_order_products['add_to_cart_order'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ffb682",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "\n",
    "It seems the values are intergers normally and probably count how many items were added to cart (need to check the documentaion for this CSV or consult a SME).  The max is 64 items, the min is 1.  This means if the cart was initilized without an item being added (there for 0 items added to cart) this may be why we have null values.  Lets check to see if there is any evidence to support the hypothsis by taking a second look at the data.  We will look to see if the order_id has any data to support there being an issue.  We would expect to see only one instance of the order_id for each null value.  \n",
    "\n",
    "The assumption here is that the data in this CSV is for each order (order_id) there are many products (product_id) that make up the order.  So if a coustomer bought 5 items, there would be 5 order_id's each with a unique product_id.  Lets group by the order_id's and count the nubmer of each Id in the dataframe, then we can compare against the IDs that have a null value.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7461a92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the dataframe by order_id and then get the value counts.\n",
    "srs_grouped_order_id = df_order_products.groupby('order_id')['order_id'].value_counts()\n",
    "print(type(srs_grouped_order_id))\n",
    "print()\n",
    "print(srs_grouped_order_id.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5c8017",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "\n",
    "An interesting point in this experiment is that we have some order_id's that have an occurence greater than 64, and that was the max for the order \"add_to_cart_order\" column.  This suggests (assuming the name of the column suggests that this is the order that items were added to the cart) that there is a limitation where if there are greater than 64 items added to the cart are recored as null values.  There may be only 7 bits of data for storing it and an error is getting thrown.  This is a matter to bring up with the SME.\n",
    "\n",
    "We will continue our exporation, but now focus on the order_ids where the count is greater than 64.  The process will be to get the unqiue list of order_id's that contain null values, then get the count of thoes ID's in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0703f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First get the unique list of order id's that have null values\n",
    "srs_order_ids_with_null = df_order_products[df_order_products['add_to_cart_order'].isnull()]['order_id'].unique()\n",
    "\n",
    "# Next we get the rows where the order id is in our unique list.  Then group by order_id and count the number of orders.\n",
    "srs_count_of_order_ids = df_order_products[df_order_products['order_id'].isin(srs_order_ids_with_null)].groupby('order_id')['order_id'].value_counts()\n",
    "\n",
    "display(srs_count_of_order_ids)\n",
    "\n",
    "# Now we check to see if there are any values less than 64.\n",
    "print(\"The number of orders less than 64:\")\n",
    "print(len(srs_count_of_order_ids[srs_count_of_order_ids <= 64]))\n",
    "\n",
    "print(\"The number of orders grater than 64:\")\n",
    "print(len(srs_count_of_order_ids[srs_count_of_order_ids > 64]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0547415c",
   "metadata": {},
   "source": [
    "#### Observatons:\n",
    "\n",
    "It seems all of the orders with null values have over 64 instances of the id in the dataframe.  It seems likely that there is some technical limitation to the system storing the \"add to cart order\" value for orders with over 64 items.  This is an issue to bring up with an SME.  For now we will replace the nulls with a large number to represent the overflow eror that has likely occured.  \n",
    "\n",
    "Also since we know more about the column, we know it does not need to be a float, so we will change it to an int."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a987d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the origional state:\n",
    "\n",
    "display(df_order_products[df_order_products.isnull()].head())\n",
    "display(type(df_order_products['add_to_cart_order'].iloc[0]))\n",
    "\n",
    "# The order products are added to the cart may have useful information even though it is missing, so we will just skip the \n",
    "#    values for now and preserve the misisng information in a way we can still process the data.\n",
    "\n",
    "# Convert datatype to int, skipping the errors\n",
    "df_order_products['add_to_cart_order'] = df_order_products['add_to_cart_order'].astype('Int64', errors='ignore')\n",
    "\n",
    "# Check for missing values and datatype\n",
    "display(df_order_products[df_order_products.isnull()].head())\n",
    "type(df_order_products['add_to_cart_order'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530fbe96",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "\n",
    "Seems the modification was successful, the dtype is int and The missing values have been properly skipped.  Time to move on to the initial annalysis of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turkish-kidney",
   "metadata": {},
   "source": [
    "# Exploritory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "short-capability",
   "metadata": {},
   "source": [
    "### Verify that the `'order_hour_of_day'` and `'order_dow'` values in the `insta_orders` tables are sensible (i.e. `'order_hour_of_day'` ranges from 0 to 23 and `'order_dow'` ranges from 0 to 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4218c93e",
   "metadata": {},
   "source": [
    "#### Goal:\n",
    "\n",
    "verify the hour range is 0-23 and day range is 0-6 for the insta_orders table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "growing-fruit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to group by the desired value then use sum to get a dataframe instead of a groupby object. We will get the index\n",
    "#      of the resulting dataframe which will be the values we need to know\n",
    "srs_range_of_hour = df_insta_orders.groupby('order_hour_of_day').sum().index\n",
    "srs_range_dow = df_insta_orders.groupby('order_dow').sum().index\n",
    "\n",
    "display(srs_range_of_hour)\n",
    "print()\n",
    "print()\n",
    "srs_range_dow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa84b53b",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "\n",
    "We can see the range of hours of the day is 0-23 as expected and the range of days in the week is 0-6 as expected.  The data passes this check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6743a70",
   "metadata": {},
   "source": [
    "### What time of day do people shop for groceries?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed653480",
   "metadata": {},
   "source": [
    "#### Goal:\n",
    "\n",
    "The first buisness question is when people are most likely to shop for groceries, We will visualize the data on time of day to see if there is a trend for when people buy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ff94e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the plot for the desired variable\n",
    "\n",
    "srs_volume_of_shoppers_per_hour = df_insta_orders['order_hour_of_day'].value_counts().sort_index()\n",
    "\n",
    "srs_volume_of_shoppers_per_hour.plot(kind='bar',\n",
    "                                     title= 'Number of Shopper per Hour',\n",
    "                                     xlabel= 'Hour of Day in 24 Hour Format',\n",
    "                                     ylabel = 'Number of Shoppers')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Show the actually values of each bar in the graph.\n",
    "srs_volume_of_shoppers_per_hour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3277b019",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "\n",
    "It seems the majority of groceries shopping happens between 7:00 AM and 9:00 PM, with the peek being between 9:00 AM and 5:00 PM.  The whole spread makes sense since that represents when people are most likely awake.  It is intresting that it is during the peak work time that the majority of shopping happens.  This may be due to the convience of online shopping as opposed to inperson shopping.  We cannot answer the latter question since we do not have data on inperson shopping.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documented-command",
   "metadata": {},
   "source": [
    "### What day of the week do people shop for groceries?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f334633b",
   "metadata": {},
   "source": [
    "#### Goal:\n",
    "\n",
    "The second business question is what day of the week most shopping occures.  This will be processed in the same way as the previous question.  A graph will be the best way to convey the findings.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd9fc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the plot for the desired variable\n",
    "\n",
    "srs_volume_of_shoppers_per_day = df_insta_orders['order_dow'].value_counts().sort_index()\n",
    "\n",
    "# Not sure why xtick does not work here I tried a list but that did not work either.\n",
    "srs_volume_of_shoppers_per_day.plot(kind='bar',\n",
    "                                     title= 'Number of Shopper per Day',\n",
    "                                     #xticks = {0:'Sunday',1:'Monday',2:'Tuesday',3:'Wednesday',4:'Thursday',5:'Friday',6:'Saturday'},\n",
    "                                     xlabel= 'Day of the Week',\n",
    "                                     ylabel = 'Number of Shoppers')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Show the actually values of each bar in the graph.\n",
    "srs_volume_of_shoppers_per_day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692ae5e3",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "\n",
    "It seems that on Sunday and Monday people are more likely to but gorceries.  There is not so great a difference to discount the other days of the week.  However, for the purposes of the buisness question Sunday and Monday are the highest density shopping day. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cutting-concert",
   "metadata": {},
   "source": [
    "### How long do people wait until placing another order?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba1c1c5",
   "metadata": {},
   "source": [
    "#### Goal:\n",
    "\n",
    "The next question to answer is how much time is there between shopping events for coustomers.  To answer this question we need to find out a few bits of information.  \n",
    "First: How many coustomers buy more than once, what is the most common number of maximum shopping events in the data set\n",
    "\n",
    "Second: How much time is there between each shopping event.  We will need to account for how many times the person has bought from us.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exciting-fantasy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We Group by the user_id and then count the number of orders each user made\n",
    "df_order_count_by_user = df_insta_orders.groupby('user_id')['user_id'].value_counts()\n",
    "df_order_count_by_user.plot(kind='hist',bins=15,xlim=[0,15])\n",
    "display(df_order_count_by_user.describe())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4d2048",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "\n",
    "It seems most people only buy once or twice.  The data is very skewed and there are some outliers, the max shopper has bought 28 times.  For the exploration for this question, we should consider how long between the first and second shopping since that describes the majority of purcheses.  We should then consider the total time between all subsiquent shoppping events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10118bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We select only the 2nd order placed and check the time between orders\n",
    "df_time_btw_first_and_second_order = df_insta_orders[df_insta_orders['order_number']==2]['days_since_prior_order']\n",
    "\n",
    "df_time_btw_first_and_second_order.plot(kind='hist')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debc64f0",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "\n",
    "It seems that most users place the 2nd order either with in a week of the first order, or after a month.  This seems to represnt 2 distinct patterns of coustomer behavaior.  One where groceries are bought weekly and one where they are bought monthly.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ebe71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now plot the behavior of all other coustomers purcheses, excluding the first and second order.\n",
    "df_time_btw_all_other_orders = df_insta_orders[(df_insta_orders['order_number']!=1) & (df_insta_orders['order_number']!=2)]['days_since_prior_order']\n",
    "\n",
    "df_time_btw_all_other_orders.plot(kind='hist')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2ae724",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "\n",
    "It seems the coustomer behavior pattern continues after the second order.  The data looks more skewed for the weekly shoppers, but this is likely due to them shopping roughly four times per month compaired to the one of monthly shoppers.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "packed-classic",
   "metadata": {},
   "source": [
    "### Is there a difference in `'order_hour_of_day'` distributions on Wednesdays and Saturdays? Plot the histograms for both days and describe the differences that you see."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01214e8",
   "metadata": {},
   "source": [
    "#### Goal:\n",
    "\n",
    "We want to check if the hour of the day orders are placed is different between weekdays and weekend.  To do this we will make two plots one for Wednesday and one for Saturday.  This will give us an overview of the patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e54e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax0,ax1) = plt.subplots(ncols=2)\n",
    "\n",
    "df_weds_orders = df_insta_orders[df_insta_orders['order_dow']==3]['order_hour_of_day']\n",
    "\n",
    "df_sat_orders = df_insta_orders[df_insta_orders['order_dow']==6]['order_hour_of_day']\n",
    "\n",
    "df_weds_orders.plot(kind='hist',ax=ax0,bins=23,title=\"Wednesday\")\n",
    "df_sat_orders.plot(kind='hist',ax=ax1,bins=23,title=\"Saturdy\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196fcc24",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "\n",
    "There seems to be very little difference in shopping habits between the days.  Both weekends and weekdays have a spike in the evening and don't tend to start until the afternoon.  There is very little latnight shopping for groceries.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3891143",
   "metadata": {},
   "source": [
    "### What's the distribution for the number of orders per customer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c26c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "srs_orders_per_coustomer = df_insta_orders.groupby('user_id')['user_id'].value_counts()\n",
    "\n",
    "srs_orders_per_coustomer.plot(kind='hist')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594ad506",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "\n",
    "It seems most of the coustomers are either first or second time orderes.  and there is a sharp drop off after the fifth/sixths order.  There is a long tail that goes out to above 28 orders, but the volume above 10 is very low.  It is important to rember that since this service is encourages recurring purchase behavior, the change over time would be a good study to see if the tail increases over time as people keep using the service. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a0319c",
   "metadata": {},
   "source": [
    "### What are the top 20 popular products (display their id and name)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d761a22",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#### Goal:\n",
    "\n",
    "We will first look at the distribution of product orders, then select the top 20 to display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "universal-facial",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of times each porduct appears in an order\n",
    "srs_products_popularity = df_order_products.groupby('product_id')['product_id'].value_counts()\n",
    "\n",
    "# Drop the extra index created by the counting process.\n",
    "srs_products_popularity = srs_products_popularity.droplevel(level=0)\n",
    "# Rename the seriers to someting descriptive so we get an accurate column name\n",
    "srs_products_popularity = srs_products_popularity.rename('order_volume')\n",
    "\n",
    "# Merge this series with the products table so we can get the product name\n",
    "df_product_pop_and_name = df_products.merge(srs_products_popularity,left_on='product_id',right_index=True)\n",
    "\n",
    "# Sort the values by order_volume and display the columns we are intrested in\n",
    "df_product_pop_and_name.sort_values(by='order_volume',ascending=False)[['product_id','product_name','order_volume']].head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98af4e13",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "\n",
    "It looks like the most popular order item is produce, specifically fruit.  This may be due to the parishable nature of these items.  It would make sense that the user has a prefrence to buy produce often, but in low volume per order, to reduce loss from spoilage.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romantic-deposit",
   "metadata": {},
   "source": [
    "### How many items do people typically buy in one order? What does the distribution look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb31d7b",
   "metadata": {},
   "source": [
    "#### Goal:\n",
    "\n",
    "We need to group by each order and then get the size of the order by counting the number of products in the order. Since each order has one entrie per product ordered,  we can count the number of times the order id appereas in the data frame.  We will plot the distribution using a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sensitive-breathing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the total number of products per order\n",
    "df_num_products_per_order = df_order_products.groupby('order_id')['order_id'].value_counts()\n",
    "\n",
    "df_num_products_per_order.droplevel(level=0)\n",
    "\n",
    "display(df_num_products_per_order.describe())\n",
    "df_num_products_per_order.plot(kind='hist',bins=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6e03a4",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "\n",
    "Most of the orders have under 20 items, but there are definatly many outliers.  The median order has about 8 items, but the maximum order has 127.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6b5537",
   "metadata": {},
   "source": [
    "### What are the top 20 items that are reordered most frequently (display their names and product IDs)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5f8098",
   "metadata": {},
   "source": [
    "#### Goal:\n",
    "\n",
    "In order to find what items are reorderd most frequently, we need to count the number of products in orders where the item is reordered.  We will first filter the orderd products dataframe for only products that are re-ordered.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "possible-change",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use a boolean mask to filter for only products taht have been re-ordered.\n",
    "df_reordered = df_order_products[df_order_products['reordered']>0]\n",
    "\n",
    "# Count the number of times each product has been re-ordered.\n",
    "srs_reordered = df_reordered.groupby('product_id')['product_id'].value_counts()\n",
    "\n",
    "# remove the duplicate index\n",
    "srs_reordered = srs_reordered.droplevel(level=0)\n",
    "# Rename the series to something that makes sense\n",
    "srs_reordered = srs_reordered.rename('reorder_volume')\n",
    "\n",
    "# Merge series into the products data set to get the name of the product and select the columns we want\n",
    "df_reordered = df_products.merge(srs_reordered,left_on='product_id',right_index=True)[['product_id','product_name','reorder_volume']]\n",
    "\n",
    "# Sort the values in decending order\n",
    "df_reordered = df_reordered.sort_values(by='reorder_volume',ascending=False)\n",
    "\n",
    "df_reordered.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550877a3",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#### Observations:\n",
    "\n",
    "It seems the most common items to be re-ordered are also the most parashiable ones.  There are mostly produce, with a significant number of fruits, and diary products.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d312b14",
   "metadata": {},
   "source": [
    "### For each product, what proportion of its orders are reorders?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738a94cf",
   "metadata": {},
   "source": [
    "#### Goal:\n",
    "\n",
    "To find the porportion of reorders we need to repeate the same steps above to find how many products are initial orders, then we can add combine that information with what we have and create a column for reorder proportion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103e7fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a boolean mask to filter for only products taht have been re-ordered.\n",
    "df_first_ordered = df_order_products[df_order_products['reordered']==0]\n",
    "\n",
    "# Count the number of times each product has been re-ordered.\n",
    "srs_first_ordered = df_first_ordered.groupby('product_id')['product_id'].value_counts()\n",
    "\n",
    "# remove the duplicate index\n",
    "srs_first_ordered = srs_first_ordered.droplevel(level=0)\n",
    "# Rename the series to something that makes sense\n",
    "srs_first_ordered = srs_first_ordered.rename('first_order_volume')\n",
    "\n",
    "# Merge series into the products data set to get the name of the product and select the columns we want\n",
    "df_first_ordered = df_products.merge(srs_first_ordered,left_on='product_id',right_index=True)[['product_id','product_name','first_order_volume']]\n",
    "\n",
    "# Make new row with the ratio.  df_reordered is the logical place to put this new column\n",
    "df_reordered['reorder_ratio'] = (df_reordered['reorder_volume']-df_first_ordered['first_order_volume'])/(df_reordered['reorder_volume']+df_first_ordered['first_order_volume'])\n",
    "\n",
    "# Sort by the ratio\n",
    "df_reordered = df_reordered.sort_values(by='reorder_ratio',ascending=False)\n",
    "\n",
    "# Display a sample\n",
    "display(df_reordered.head(20))\n",
    "\n",
    "df_reordered['reorder_ratio'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221f022f",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "\n",
    "It seems the products with the higest re-order ratio have suprisingly low total re-roder volume.  This is likely that a few coustomers have faviorate products that can be consumed before the next order.  Most products however seem to be more likely to be first ordered, as shown with the negitive median for the ratio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subject-boxing",
   "metadata": {},
   "source": [
    "### For each customer, what proportion of their products ordered are reorders?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697027fd",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#### Goal:\n",
    "\n",
    "We will need to group each order by coustomer, then create a column for first_orders and re_orders.  We can then find the ratio per coustomer across all their orders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b212c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the user_id column into the order_products dataframe so we can have all of the information we need in one place\n",
    "df_user_order_products = df_order_products.merge(df_insta_orders, left_on='order_id',right_on='order_id')\n",
    "\n",
    "# select only the columns we need\n",
    "df_user_order_products = df_user_order_products[['order_id','user_id','product_id','reordered']]\n",
    "\n",
    "# Use a piviot table to get the sum of reorders, since the values are 1 or 0 this will be the total number of reorders by user\n",
    "df_reorder_counts = pd.pivot_table(df_user_order_products,values=['reordered'],index=['user_id'],aggfunc='sum')\n",
    "\n",
    "# Use a piviot table to get the total orders per user\n",
    "df_total_orders = pd.pivot_table(df_user_order_products,values=['reordered'],index=['user_id'],aggfunc='count')\n",
    "\n",
    "# Subtract the re-orders from the total to get the first orders \n",
    "df_first_orders = df_total_orders-df_reorder_counts\n",
    "\n",
    "# Rename the column to something that makes sense\n",
    "df_first_orders.rename(columns={'reordered':'first_orders'},inplace=True)\n",
    "\n",
    "df_proportions = df_reorder_counts/df_total_orders\n",
    "\n",
    "display(df_proportions.head(10))\n",
    "\n",
    "df_proportions.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda14b7f",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "\n",
    "There seems to be a wide variation in reorders.  The data is likely very skewed because all first orders for a given user has no re-orders by deffinition.  Regardless the median seems to be about 50% reorders so most coustomers reorder about half of there products.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d35137",
   "metadata": {},
   "source": [
    "### What are the top 20 items that people put in their carts first? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a1d295",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#### Goal:\n",
    "\n",
    "We need to sort the products in the order_products dataframe by only the items put in the cart first.  We can then group by each product and get the counts.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forty-netherlands",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select only items that were added to the cart first\n",
    "df_first_items = df_order_products[df_order_products['add_to_cart_order']==1]\n",
    "\n",
    "# Get a count of each product\n",
    "srs_first_item_counts = df_first_items.groupby('product_id')['product_id'].value_counts()\n",
    "\n",
    "# Clean up the dataframe produced and rename it to something that makes sense\n",
    "srs_first_item_counts = srs_first_item_counts .droplevel(level=0)\n",
    "srs_first_item_counts = srs_first_item_counts .rename('first_item_counts')\n",
    "\n",
    "# Merge to the producst data frame so we can get the name of the product as well\n",
    "df_first_item_counts = df_products.merge(srs_first_item_counts,left_on='product_id',right_index=True)[['product_id','product_name','first_item_counts']]\n",
    "\n",
    "# Sort the data the counts decending \n",
    "df_first_item_counts = df_first_item_counts.sort_values(by='first_item_counts',ascending=False)\n",
    "display(df_first_item_counts.head(20))\n",
    "df_first_item_counts['first_item_counts'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662e97d1",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "\n",
    "It seems the items most often re-orderd are very similar to the items added to the cart first.  It strikes me once again that bannanas are the most common item.  There is also a very large drop off in the items counts after the bannanas, the next most common is only about half as common.  By the 20th product the count has dropped to less than one tenth the count of the most common.   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "01a7be9ec63e704a62cefc5fe7a4756944464ee731be31632bdf42a4cb4688cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
